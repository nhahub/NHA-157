{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3ef037",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 in Colab (End-to-End)\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Downloading a **small dataset**.\n",
    "2. Preprocessing with **NLTK** and **spaCy** (tokenization, stopword removal, lemmatization).\n",
    "3. Fine-tuning GPT-2 with **PyTorch + Hugging Face**.\n",
    "4. Testing the fine-tuned model with text generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7be8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -qU transformers datasets torch nltk spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b068179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load small dataset (wikitext-2)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Take only a small subset of the training set for quick fine-tuning\n",
    "small_train = dataset[\"train\"].shuffle(seed=42).select(range(200))\n",
    "small_val = dataset[\"validation\"].shuffle(seed=42).select(range(50))\n",
    "\n",
    "print(small_train[0])\n",
    "print(small_val[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc\n",
    "              if not token.is_punct and not token.is_space and token.text.lower() not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(lambda ex: {\"clean_text\": preprocess(ex[\"text\"])})\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84743b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"clean_text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\",\"clean_text\"])\n",
    "tokenized_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "print(tokenized_ds['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a61720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-small-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"Artificial intelligence is\"\n",
    "outputs = generator(prompt, max_new_tokens=40, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "print(\"Generated text:\\n\", outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save fine-tuned model locally\n",
    "model.save_pretrained(\"./final_gpt2_model\")\n",
    "tokenizer.save_pretrained(\"./final_gpt2_model\")\n",
    "print(\"Model saved to ./final_gpt2_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
