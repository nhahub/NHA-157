{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: Setup and Installs ---\n",
    "# Install the necessary libraries\n",
    "!pip install transformers datasets torch huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from google.colab import drive\n",
    "import os\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, \n",
    "    GPT2LMHeadModel, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from huggingface_hub import notebook_login, HfFolder # <--- NEW IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: Configuration and Login ---\n",
    "# 1. Log in to the Hugging Face Model Hub (it will prompt for your token)\n",
    "print(\"Logging into Hugging Face Hub...\")\n",
    "notebook_login() # A browser pop-up will appear; enter your User Access Token.\n",
    "\n",
    "# 2. Define your desired Model ID/Repository name\n",
    "# IMPORTANT: REPLACE 'YOUR_HF_USERNAME' with your actual username/org name\n",
    "HF_REPO_ID = \"EhabBelllkasy01/gpt2-all-recipes\" \n",
    "print(f\"Checkpoints will be pushed to: https://huggingface.co/{HF_REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define a local path (required by Trainer for temporary file storage)\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/GPT2_Recipe_Temp_Files\"\n",
    "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
    "print(f\"Temporary files will be saved locally to: {DRIVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e877871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: Load Tokenizer and Add Custom Tokens ---\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define and Add Custom Special Tokens for recipe structure\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': [\n",
    "        '<|title|>', \n",
    "        '<|ingredients|>', \n",
    "        '<|instructions|>', \n",
    "        '<|endofrecipe|>'\n",
    "    ]\n",
    "}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 4: Load Model and Resize Embeddings ---\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer)) \n",
    "print(f\"Added {num_added_toks} new tokens and resized model embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 5: Load, Format, and Tokenize Dataset ---\n",
    "print(\"\\nLoading dataset...\")\n",
    "raw_datasets = load_dataset('corbt/all-recipes', split='train[:5%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenization function\n",
    "def tokenize_and_format(examples):\n",
    "    return tokenizer(\n",
    "        examples['input'], \n",
    "        truncation=True, \n",
    "        max_length=512,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_and_format, batched=True, remove_columns=[\"input\"])\n",
    "split_datasets = tokenized_datasets.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "\n",
    "print(f\"Training on {len(train_dataset)} samples, validating on {len(eval_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc101f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 6: Configure Training and Checkpointing ---\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Configure Training Arguments for Hub Push and Resuming\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=DRIVE_PATH,                  # Local directory for temporary saves/logs\n",
    "    num_train_epochs=5,                     \n",
    "    per_device_train_batch_size=4,          \n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,                     \n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{DRIVE_PATH}/logs',\n",
    "    logging_steps=50,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # evaluation_strategy=\"steps\", \n",
    "    eval_strategy=\"steps\", # Evaluate every 'eval_steps' (FIXED NAME) \n",
    "    eval_steps=500,\n",
    "    \n",
    "    # --- HUB CHECKPOINTING AND PUSH ---\n",
    "    save_strategy=\"steps\",              # Save on steps, not epochs\n",
    "    save_steps=1000,                    # Save and push a checkpoint every 1000 training steps\n",
    "    save_total_limit=1,                 # Keep only the latest local checkpoint\n",
    "    push_to_hub=True,                   # CRITICAL: Enable pushing to the Hub\n",
    "    hub_model_id=HF_REPO_ID,            # Specify the repository name\n",
    "    hub_token=HfFolder.get_token(),     # Use the token from the login\n",
    "    hub_private_repo=False,             # Set to True for a private repo\n",
    ")\n",
    "\n",
    "# --- STEP 7: Initialize Trainer and Train ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer, # Pass tokenizer for Hub auto-upload\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7be54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting training...\")\n",
    "print(\"Checkpoints will be pushed to the Hugging Face Hub automatically.\")\n",
    "# The Trainer checks the Hub for the latest checkpoint of the specified HF_REPO_ID\n",
    "trainer.train(\n",
    "    # resume_from_checkpoint=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 8: Save Final Model and Push ---\n",
    "# Trainer automatically handles the final save and push when push_to_hub=True\n",
    "print(f\"\\nFinal fine-tuned model has been pushed to the Hugging Face Hub under: {HF_REPO_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
