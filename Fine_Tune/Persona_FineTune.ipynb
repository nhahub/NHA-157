{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ðŸ¤– Full GPT-2 Fine-Tuning Program in Colab\n",
    "\n",
    "# --- 1. SETUP AND INSTALLATION ---\n",
    "# ----------------------------------\n",
    "print(\"--- 1. Installing Libraries ---\")\n",
    "! pip install transformers datasets accelerate -q\n",
    "# Install the latest version of accelerate and transformers for best compatibility\n",
    "! pip install -U accelerate transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    GPT2LMHeadModel, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c5851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "MODEL_ID = \"EhabBelllkasy01/gpt2-all-recipes\"\n",
    "DATASET_NAME = \"google/Synthetic-Persona-Chat\"\n",
    "SAVE_DIR = \"./gpt2-persona-chat-finetuned-from-recipes\"\n",
    "BLOCK_SIZE = 128 # Max sequence length for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. LOAD MODEL AND TOKENIZER ---\n",
    "# ------------------------------------\n",
    "print(\"\\n--- 2. Loading Model and Tokenizer ---\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL_ID)\n",
    "    print(f\"âœ… Loaded model: {MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Set pad token: GPT-2 tokenizer often requires this for training\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e877871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. LOAD AND PREPARE DATASET ---\n",
    "# ------------------------------------\n",
    "print(\"\\n--- 3. Loading and Preparing Dataset ---\")\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "# Use the training split\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "def format_conversation(example):\n",
    "    \"\"\"\n",
    "    Formats the conversation and personas into a single sequence of text.\n",
    "    Format: P1: [Persona 1] P2: [Persona 2] <|startofchat|> [Turn 1] <|eos|> [Turn 2] <|eos|>\n",
    "    \"\"\"\n",
    "    # Combine personas\n",
    "    persona_1 = \" \".join(example[\"user 1 personas\"])\n",
    "    persona_2 = \" \".join(example[\"user 2 personas\"])\n",
    "    personas = f\"P1: {persona_1} P2: {persona_2}\"\n",
    "    \n",
    "    # Concatenate the conversation turns, using the EOS token as a separator\n",
    "    conversation = tokenizer.eos_token.join(example[\"Best Generated Conversation\"])\n",
    "    \n",
    "    # Combine everything. The final EOS token is crucial for training the model \n",
    "    # to understand where a conversation sequence ends.\n",
    "    full_text = f\"{personas} <|startofchat|> {conversation} {tokenizer.eos_token}\"\n",
    "    return {\"text\": full_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the formatting function to the dataset\n",
    "processed_dataset = train_dataset.map(\n",
    "    format_conversation, \n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "print(f\"âœ… Dataset examples formatted. Total examples: {len(processed_dataset)}\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenizes the formatted text, truncating to BLOCK_SIZE.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=BLOCK_SIZE,\n",
    "        padding=\"max_length\" # Pad to max_length for consistent batching\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the processed dataset\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=os.cpu_count(), # Use all available cores for fast tokenization\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Data collator for Causal Language Modeling (CLM)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False # We use CLM for GPT-2, not Masked LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. FINE-TUNING THE MODEL ---\n",
    "# ---------------------------------\n",
    "print(\"\\n--- 4. Starting Fine-Tuning ---\")\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3, # Recommended starting point\n",
    "    per_device_train_batch_size=4, # Adjust based on GPU memory (4-8 is common for Colab)\n",
    "    gradient_accumulation_steps=4, # Effectively increases batch size to 16 (4*4)\n",
    "    learning_rate=5e-5,\n",
    "    save_strategy=\"epoch\", # Save checkpoint at the end of each epoch\n",
    "    logging_steps=500,\n",
    "    report_to=\"none\", # Disable reporting to external services\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision if a GPU is available for faster training\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc101f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training! This will take time depending on the GPU.\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n--- Fine-Tuning Complete! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7be54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. SAVE AND TEST THE NEW MODEL ---\n",
    "# ---------------------------------------\n",
    "print(\"\\n--- 5. Saving and Testing the New Model ---\")\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer to the new local directory\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR) # Save the tokenizer too, especially its special tokens\n",
    "\n",
    "print(f\"âœ… New fine-tuned model and tokenizer saved locally to: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model for testing\n",
    "from transformers import pipeline\n",
    "try:\n",
    "    new_model = GPT2LMHeadModel.from_pretrained(SAVE_DIR)\n",
    "    generator = pipeline(\n",
    "        'text-generation', \n",
    "        model=new_model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
    "    )\n",
    "    \n",
    "    # Test prompt\n",
    "    test_prompt = \"P1: I enjoy collecting antique books and reading mysteries. P2: I work as a chef and love making pasta. <|startofchat|> P1: I just finished a great book about a famous detective. What have you been up to?\"\n",
    "\n",
    "    print(f\"\\n--- Testing with Prompt ---\\nPROMPT: {test_prompt}\")\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generator(\n",
    "        test_prompt, \n",
    "        max_length=150, \n",
    "        num_return_sequences=1,\n",
    "        do_sample=True, \n",
    "        temperature=0.8, # Adjust temperature for creativity\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.2,\n",
    "    )[0]['generated_text']\n",
    "\n",
    "    print(\"\\n--- GENERATED RESPONSE ---\")\n",
    "    # Clean up the output to only show the response part\n",
    "    response_text = generated_text[len(test_prompt):].strip()\n",
    "    # Find the first EOS token and stop there for a clean response\n",
    "    response_text = response_text.split(tokenizer.eos_token)[0].strip()\n",
    "    \n",
    "    print(response_text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during testing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c849e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9b9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186eeec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c37b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8405f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1e44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ee5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce298e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd6b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c9efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb604a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229f417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183cc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## ðŸ¤– Full GPT-2 Fine-Tuning Program in Colab\n",
    "\n",
    "# --- 1. SETUP AND INSTALLATION ---\n",
    "# ----------------------------------\n",
    "print(\"--- 1. Installing Libraries ---\")\n",
    "! pip install transformers datasets accelerate -q\n",
    "# Install the latest version of accelerate and transformers for best compatibility\n",
    "! pip install -U accelerate transformers -q\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    GPT2LMHeadModel, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import os\n",
    "\n",
    "# Define constants\n",
    "MODEL_ID = \"EhabBelllkasy01/gpt2-all-recipes\"\n",
    "DATASET_NAME = \"google/Synthetic-Persona-Chat\"\n",
    "SAVE_DIR = \"./gpt2-persona-chat-finetuned-from-recipes\"\n",
    "BLOCK_SIZE = 128 # Max sequence length for tokenization\n",
    "\n",
    "\n",
    "# --- 2. LOAD MODEL AND TOKENIZER ---\n",
    "# ------------------------------------\n",
    "print(\"\\n--- 2. Loading Model and Tokenizer ---\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL_ID)\n",
    "    print(f\"âœ… Loaded model: {MODEL_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Set pad token: GPT-2 tokenizer often requires this for training\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "# --- 3. LOAD AND PREPARE DATASET ---\n",
    "# ------------------------------------\n",
    "print(\"\\n--- 3. Loading and Preparing Dataset ---\")\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "\n",
    "# Use the training split\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "def format_conversation(example):\n",
    "    \"\"\"\n",
    "    Formats the conversation and personas into a single sequence of text.\n",
    "    Format: P1: [Persona 1] P2: [Persona 2] <|startofchat|> [Turn 1] <|eos|> [Turn 2] <|eos|>\n",
    "    \"\"\"\n",
    "    # Combine personas\n",
    "    persona_1 = \" \".join(example[\"User 1 Personas\"])\n",
    "    persona_2 = \" \".join(example[\"User 2 Personas\"])\n",
    "    personas = f\"P1: {persona_1} P2: {persona_2}\"\n",
    "    \n",
    "    # Concatenate the conversation turns, using the EOS token as a separator\n",
    "    conversation = tokenizer.eos_token.join(example[\"Conversation\"])\n",
    "    \n",
    "    # Combine everything. The final EOS token is crucial for training the model \n",
    "    # to understand where a conversation sequence ends.\n",
    "    full_text = f\"{personas} <|startofchat|> {conversation} {tokenizer.eos_token}\"\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "# Map the formatting function to the dataset\n",
    "processed_dataset = train_dataset.map(\n",
    "    format_conversation, \n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "print(f\"âœ… Dataset examples formatted. Total examples: {len(processed_dataset)}\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenizes the formatted text, truncating to BLOCK_SIZE.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=BLOCK_SIZE,\n",
    "        padding=\"max_length\" # Pad to max_length for consistent batching\n",
    "    )\n",
    "\n",
    "# Tokenize the processed dataset\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=os.cpu_count(), # Use all available cores for fast tokenization\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Data collator for Causal Language Modeling (CLM)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False # We use CLM for GPT-2, not Masked LM\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. FINE-TUNING THE MODEL ---\n",
    "# ---------------------------------\n",
    "print(\"\\n--- 4. Starting Fine-Tuning ---\")\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3, # Recommended starting point\n",
    "    per_device_train_batch_size=4, # Adjust based on GPU memory (4-8 is common for Colab)\n",
    "    gradient_accumulation_steps=4, # Effectively increases batch size to 16 (4*4)\n",
    "    learning_rate=5e-5,\n",
    "    save_strategy=\"epoch\", # Save checkpoint at the end of each epoch\n",
    "    logging_steps=500,\n",
    "    report_to=\"none\", # Disable reporting to external services\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision if a GPU is available for faster training\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Start training! This will take time depending on the GPU.\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n--- Fine-Tuning Complete! ---\")\n",
    "\n",
    "\n",
    "# --- 5. SAVE AND TEST THE NEW MODEL ---\n",
    "# ---------------------------------------\n",
    "print(\"\\n--- 5. Saving and Testing the New Model ---\")\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer to the new local directory\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR) # Save the tokenizer too, especially its special tokens\n",
    "\n",
    "print(f\"âœ… New fine-tuned model and tokenizer saved locally to: {SAVE_DIR}\")\n",
    "\n",
    "# Load the saved model for testing\n",
    "from transformers import pipeline\n",
    "try:\n",
    "    new_model = GPT2LMHeadModel.from_pretrained(SAVE_DIR)\n",
    "    generator = pipeline(\n",
    "        'text-generation', \n",
    "        model=new_model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
    "    )\n",
    "    \n",
    "    # Test prompt\n",
    "    test_prompt = \"P1: I enjoy collecting antique books and reading mysteries. P2: I work as a chef and love making pasta. <|startofchat|> P1: I just finished a great book about a famous detective. What have you been up to?\"\n",
    "\n",
    "    print(f\"\\n--- Testing with Prompt ---\\nPROMPT: {test_prompt}\")\n",
    "\n",
    "    # Generate text\n",
    "    generated_text = generator(\n",
    "        test_prompt, \n",
    "        max_length=150, \n",
    "        num_return_sequences=1,\n",
    "        do_sample=True, \n",
    "        temperature=0.8, # Adjust temperature for creativity\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.2,\n",
    "    )[0]['generated_text']\n",
    "\n",
    "    print(\"\\n--- GENERATED RESPONSE ---\")\n",
    "    # Clean up the output to only show the response part\n",
    "    response_text = generated_text[len(test_prompt):].strip()\n",
    "    # Find the first EOS token and stop there for a clean response\n",
    "    response_text = response_text.split(tokenizer.eos_token)[0].strip()\n",
    "    \n",
    "    print(response_text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during testing: {e}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
